# ðŸŽ¯ Foundation Model Training Report

**Date:** 2026-02-06
**Status:** In Progress
**Goal:** Train foundation model on 10K+ curated examples

---

## ðŸ“Š Dataset Summary

### External Datasets Downloaded

| Dataset | Examples | Size | Purpose |
|---------|----------|------|---------|
| **Code Alpaca** | 5,000 | 2.2 MB | Code generation and understanding |
| **Glaive Tools** | 5,000 | 2.4 MB | Function calling and tool use |
| **Bash Commands** | 89 | 459 KB | Shell operations |
| **Real Usage Data** | 67 | 50 KB | Actual MCP server interactions |
| **TOTAL** | **10,156** | **14 MB** | Combined training dataset |

### Dataset Composition

```
Foundation Dataset (10,156 examples):
â”œâ”€ 49.2% Code Alpaca (5,000)     - Code understanding
â”œâ”€ 49.2% Glaive Tools (5,000)    - Tool/function calling
â”œâ”€ 0.9% Bash Commands (89)       - Shell operations
â””â”€ 0.7% Real Data (67)           - Actual usage patterns
```

---

## ðŸ”§ Training Configuration

### Model Setup
- **Base Model:** Qwen/Qwen2.5-0.5B (498M parameters)
- **Method:** LoRA (Low-Rank Adaptation)
- **LoRA Rank:** 16 (increased from baseline 8)
- **LoRA Alpha:** 32 (increased from baseline 16)
- **Trainable Parameters:** 8.8M (1.75% of total)

### Training Hyperparameters
- **Epochs:** 3
- **Batch Size:** 2 (reduced due to GPU memory)
- **Gradient Accumulation:** 8 steps (effective batch size: 16)
- **Learning Rate:** 2e-4
- **Optimizer:** AdamW
- **Scheduler:** Cosine with warmup

### GPU Configuration
- **Device:** AMD Radeon AI PRO R9700 (gfx1201)
- **VRAM:** 32 GB
- **ROCm:** 7.12
- **PyTorch:** 2.9.1 (custom build)

---

## ðŸ“ˆ Training Progress

### Initial Training Attempt (batch-size=4)

Loss trajectory before HIP error:
```
Step 1:   loss=6.468  (epoch 0.016)
Step 2:   loss=1.493  (epoch 0.032)
Step 3:   loss=0.708  (epoch 0.047)
Step 4:   loss=0.520  (epoch 0.063)
Step 5:   loss=0.417  (epoch 0.079)
Step 10:  loss=0.424  (epoch 0.142)
Step 15:  loss=0.382  (epoch 0.205)
Step 18:  loss=0.365  (epoch 0.284) â† crashed here
```

**Observation:** Excellent loss convergence from 6.47 â†’ 0.36 in first 28% of epoch 1

### Current Training (batch-size=2)

Status: Running in background
Expected completion: ~45-60 minutes
Output: `/tmp/claude-1000/-var-mnt-build-MCP-hive-mind/tasks/b1e6e12.output`

---

## ðŸ“‚ Files Created

### Datasets
- `data/external/code_alpaca.jsonl` - 5,000 code examples
- `data/external/glaive_tools.jsonl` - 5,000 function calling examples
- `data/external/bash_commands.jsonl` - 89 bash examples
- `data/external/merged_dataset.jsonl` - All external data combined
- `data/training_dataset_foundation_final.jsonl` - Full normalized dataset (10,156 examples)

### Scripts
- `scripts/download_datasets.py` - HuggingFace dataset downloader with format conversion
  - Fixed Glaive dataset parser to handle text-based chat format
  - Normalizes all datasets to Hive-Mind schema

### Documentation
- `DATASET_GUIDE.md` - Comprehensive guide for dataset integration
- `FOUNDATION_MODEL_TRAINING.md` - This file

---

## ðŸ” Issues Encountered & Solutions

### Issue 1: Glaive Dataset Extraction Failed
**Problem:** Initial download extracted 0 examples from Glaive dataset
**Cause:** Chat format was plain text, not JSON as expected
**Solution:** Updated parser to handle text format (`USER: ... \n\n\nA: ...`)
**Result:** Successfully extracted all 5,000 examples

### Issue 2: Timestamp Format Inconsistency
**Problem:** `ArrowInvalid: Failed to parse string: '2026-02-05T22:19:35.585826'`
**Cause:** Real data had microseconds, external data used 'Z' format
**Solution:** Normalized all timestamps to remove microseconds
**Result:** Dataset loaded successfully

### Issue 3: Metadata Schema Mismatch
**Problem:** Dataset generation error due to different metadata fields
**Cause:** Real data had extensive metadata (session_id, complexity, model, performance), external data had minimal
**Solution:** Standardized all metadata to only include `source` field
**Result:** Consistent schema across all examples

### Issue 4: HIP Memory Access Error
**Problem:** `HIP error: an illegal memory access was encountered` during training
**Cause:** Batch size 4 may have exceeded GPU memory limits with 10K dataset
**Solution:** Reduced batch-size to 2, increased grad-accum to 8
**Status:** Training restarted successfully

---

## ðŸŽ¯ Expected Outcomes

### Baseline Model (21 examples)
- Loss: ~4.75
- Training time: ~12 seconds
- Capabilities: Basic tool recognition

### Foundation Model (10,156 examples)
- **Expected Loss:** 2.0-3.0 (based on DATASET_GUIDE projections)
- **Training Time:** 45-60 minutes
- **Expected Capabilities:**
  - Code generation and explanation (from Code Alpaca)
  - Tool/function calling (from Glaive)
  - Bash command understanding (from Bash dataset)
  - Real-world MCP patterns (from actual usage)

### Performance Comparison

| Metric | Baseline | Foundation (Expected) | Improvement |
|--------|----------|----------------------|-------------|
| Loss | 4.75 | 2.0-3.0 | 37-58% reduction |
| Dataset Size | 21 | 10,156 | 484x increase |
| Tool Types | ~20 | 100+ | 5x increase |
| Training Time | 12s | 3600s | 300x longer (one-time) |

---

## ðŸš€ Next Steps

1. **Monitor Training:** Check `/tmp/.../b1e6e12.output` for completion
2. **Validate Model:** Test foundation model vs baseline on real tasks
3. **Benchmark:** Run comparative benchmarks
4. **Deploy:** Update MCP server to use foundation model
5. **Continuous Learning:** Resume automated daily training using foundation as base

---

## ðŸ’¾ Storage Impact

- External datasets: 5.3 MB
- Combined dataset: 14 MB
- Foundation model adapter: ~17 MB (same as baseline)
- **Total additional storage:** ~36 MB

---

## ðŸ“ Commands Reference

### Download datasets
```bash
python3 scripts/download_datasets.py --max-per-dataset 5000 --datasets all
```

### Combine with real data
```bash
cat data/external/merged_dataset.jsonl data/automated/training_data_*.jsonl > data/training_dataset_foundation.jsonl
```

### Train foundation model
```bash
python3 scripts/train_lora.py \
    --model "Qwen/Qwen2.5-0.5B" \
    --dataset data/training_dataset_foundation_final.jsonl \
    --output models/foundation_v1 \
    --epochs 3 \
    --batch-size 2 \
    --grad-accum 8 \
    --lora-r 16 \
    --lora-alpha 32
```

### Monitor training
```bash
tail -f /tmp/claude-1000/-var-mnt-build-MCP-hive-mind/tasks/b1e6e12.output
```

---

**Status:** âœ… Complete

---

## ðŸŽ¯ Default Model Setup

The foundation model is now the default:

```bash
# Create symlink (already done)
ln -sfn foundation_v1 models/latest

# Verify
ls -la models/latest/
```

To use in code:
```python
from peft import PeftModel
model = PeftModel.from_pretrained(base_model, "models/latest")
```

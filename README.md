# ğŸ Hive-Mind

> **Distributed AI Memory System with Dual-Mode Access: HTTP API + MCP Protocol**

[![Redis](https://img.shields.io/badge/Redis-7.4.7-DC382D?logo=redis&logoColor=white)](https://redis.io/)
[![Python](https://img.shields.io/badge/Python-3.14-3776AB?logo=python&logoColor=white)](https://www.python.org/)
[![ROCm](https://img.shields.io/badge/ROCm-7.12.0-FF6600?logo=amd&logoColor=white)](https://rocm.docs.amd.com/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

**Built in one incredible session on 2026-02-01** ğŸš€
**Dual-mode HTTP API added 2026-02-03** âš¡
**LoRA Training validated 2026-02-05** ğŸ§  *([PyTorch 2.9.1 + ROCm 7.12](learning-pipeline/TRAINING_RESULTS.md))*

---

## ğŸ“– The Journey

Started with a simple question: *"How do we fix context loss?"*

Ended with a **production-ready distributed AI memory system** with **dual-mode access**:
- âœ… Survives terminal restarts
- âœ… Shares context across machines
- âœ… Caches expensive operations
- âœ… Learns from interactions
- âœ… Scales horizontally
- âœ… Never forgets
- âœ… **HTTP API for Open Interpreter & any tool**
- âœ… **MCP Protocol for Claude Code**
- âœ… **Cross-tool context sharing**

### What We Built

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ HIVE-MIND                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Redis      â”‚  â”‚   Redis      â”‚  â”‚   Redis      â”‚ â”‚
â”‚  â”‚  Master :7000â”‚  â”‚  Master :7001â”‚  â”‚  Master :7002â”‚ â”‚
â”‚  â”‚  Slots 0-5k  â”‚  â”‚  Slots 5k-10kâ”‚  â”‚  Slots 10k-16kâ”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                 â”‚                 â”‚          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Replica    â”‚  â”‚   Replica    â”‚  â”‚   Replica    â”‚ â”‚
â”‚  â”‚   :7003      â”‚  â”‚   :7004      â”‚  â”‚   :7005      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Redis Sentinel (Quorum 2/3)                    â”‚   â”‚
â”‚  â”‚  Ports: 26379, 26380, 26381                     â”‚   â”‚
â”‚  â”‚  Auto-failover: < 10s                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  HTTP API (Port 8090)â”‚  â”‚  MCP Server (stdio)  â”‚   â”‚
â”‚  â”‚  â€¢ REST Endpoints    â”‚  â”‚  â€¢ Session Mgmt      â”‚   â”‚
â”‚  â”‚  â€¢ FastAPI/Uvicorn   â”‚  â”‚  â€¢ Tool Caching      â”‚   â”‚
â”‚  â”‚  â€¢ Systemd Service   â”‚  â”‚  â€¢ Learning Queue    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                           â”‚                â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                       â”‚                                â”‚
â”‚              Shared Redis Backend                      â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ llama-server â”‚              â”‚ llama-server â”‚       â”‚
â”‚  â”‚ Qwen2.5-7B   â”‚              â”‚  Qwen3-8B    â”‚       â”‚
â”‚  â”‚ Port :8080   â”‚              â”‚ Port :8088   â”‚       â”‚
â”‚  â”‚ 89 tok/s âš¡  â”‚              â”‚ 74 tok/s âš¡  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                   â”‚
        â–¼                                   â–¼
   Open Interpreter                    Claude Code
   Python Scripts                      (MCP Protocol)
   Any HTTP Client                     Future: DELL
```

---

## âš¡ Performance Benchmarks

### ğŸ“Š Redis Cluster

| Operation | Ops/Second | Latency |
|-----------|------------|---------|
| **SET** | 10,655 | < 1ms |
| **GET** | 14,728 | < 1ms |
| **HSET** | 11,308 | < 1ms |
| **Mixed (70R/30W)** | 12,720 | < 1ms |
| **Pipelined** | **59,763** âš¡ | < 1ms |

**Average**: 12,701 ops/sec  
**Peak**: 59,763 ops/sec (pipelined)

### ğŸ¦™ LLM Inference

| Model | Port | Tokens/Sec | VRAM |
|-------|------|------------|------|
| **Qwen2.5-Coder-7B** | 8080 | 89.0 tok/s âš¡ | 4.2 GB |
| **Qwen3-8B** | 8088 | 74.4 tok/s âš¡ | 5.0 GB |

**Total VRAM**: 11.2 GB / 32 GB (65% free)
**Headroom**: Can fit 30B model simultaneously!

### ğŸ Dual-Mode Access

| Mode | Protocol | Performance | Use Case |
|------|----------|-------------|----------|
| **HTTP API** | REST (Port 8090) | ~5ms latency, 1000+ req/s | Open Interpreter, Scripts, External Tools |
| **MCP Protocol** | stdio | < 1ms latency, 5000+ ops/s | Claude Code Integration |

**MCP Operations**:
- memory_store: 6,260 ops/s
- memory_recall: 9,733 ops/s
- tool_cache_set: 8,140 ops/s
- tool_cache_get: 9,798 ops/s

### ğŸ§  Learning Pipeline (Phase 4) âœ… OPERATIONAL

**PyTorch 2.9.1 + ROCm 7.12** - [Build Story](https://github.com/tlee933/TheRock-Forge-EXPERIMENTAL/tree/fedora-atomic-rocm7.12-ai-pro-experimental/external-builds/pytorch/JOURNEY.md)

| Model | Dataset | Loss | Time | Throughput |
|-------|---------|------|------|------------|
| **Qwen2.5-0.5B** | 1,500 samples | 0.34 | 9 min | 8.2 samples/s |
| **Qwen2.5-Coder-7B** | 10,156 samples | **0.30** | 1h 43min | 4.9 samples/s |

**Features**: LoRA fine-tuning, GGUF export, TorchAO quantization, Zero HIP errors âœ¨

### ğŸ§  Smart Optimizer (Phase 4.5) âœ… NEW

Auto-selects optimal configuration based on model, hardware, and quality requirements:

```bash
python scripts/auto_optimize.py --model "Qwen/Qwen2.5-Coder-7B" --task training --quality balanced
```

| Detection | Auto-Config |
|-----------|-------------|
| GPU arch (gfx1201) | LoRA rank (r=8/16/32) |
| VRAM (34 GB) | Batch size + grad accum |
| BF16 support | Precision (bf16/fp16) |
| Flash Attention | Quantization (int4/int8/Q4_K_M) |

**Quality Modes**: `fast` (speed) â†’ `balanced` â†’ `best` (quality)

See [`learning-pipeline/TRAINING_RESULTS.md`](learning-pipeline/TRAINING_RESULTS.md) for full details.

---

## ğŸ¯ Features

### ğŸ”Œ Dual-Mode Access (NEW!)
- **HTTP API (Port 8090)**: RESTful access for Open Interpreter, scripts, any tool
- **MCP Protocol (stdio)**: Native integration for Claude Code
- **Cross-Tool Sharing**: Context stored via HTTP is accessible via MCP and vice versa
- **Systemd Service**: HTTP API auto-starts on boot
- **Python Client**: Easy integration with `hivemind_client.py`
- **Interactive Docs**: Auto-generated Swagger UI at `/docs`

### ğŸ’¾ Distributed Memory
- **Persistent Sessions**: Context survives terminal restarts
- **Multi-Machine**: Share memory across BEAST, DELL, and future nodes
- **Auto-Sharding**: 16,384 hash slots across 3 masters
- **High Availability**: 3 replicas + Sentinel auto-failover

### ğŸš€ Smart Caching
- **Tool Output Cache**: 8,140 writes/sec, 9,798 reads/sec
- **TTL-based Expiry**: Configurable per cache type
- **Cluster-Wide**: All nodes share the same cache

### ğŸ§  Learning Pipeline (Phase 4) ğŸ”¥ **NEW!**
- **1,500 Training Samples**: Comprehensive Fedora bootc + Linux + AI expertise
- **LoRA Fine-tuning**: 1.05% trainable params (80.7M / 7.6B)
- **BF16 Precision**: 125 TFLOPS on gfx1201 (RDNA 4)
- **On-the-fly Tokenization**: Memory-efficient training pipeline
- **Native Performance**: TheRock ROCm 7.12 (+19% vs generic container)
- **Status**: 85% complete (training pipeline proven, ROCm compat fix needed)

### ğŸ”’ Production-Ready
- **AOF + RDB Persistence**: No data loss
- **Automatic Failover**: < 10s recovery
- **Password Authentication**: Secured cluster
- **Resource Efficient**: 2.93 MB memory used, 12 GB available

---

## ğŸš€ Quick Start

### Prerequisites

```bash
# System
- Docker (for Redis cluster)
- Python 3.14+
- ROCm 7.12+ (for GPU inference)

# Hardware
- AMD GPU with 8GB+ VRAM (tested on R9700 XT 32GB)
- 16GB+ RAM recommended
- SSD storage
```

### Installation

```bash
# Clone the repo
git clone https://github.com/tlee933/hive-mind.git
cd hive-mind

# Deploy Redis Cluster (6 nodes + 3 Sentinels)
./scripts/deploy-redis-cluster.sh

# Set up Python environment
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# Configure (copy example and set your password)
cp config.example.yaml config.yaml
# Edit config.yaml with your Redis password

# Test everything
python3 tests/test-hive-mind-stack.py

# Run performance benchmarks
python3 tests/benchmark-hive-mind.py
```

### Start Services

```bash
# Start Redis cluster (if not running)
docker ps | grep redis  # Should show 9 containers

# Start llama-servers (optional)
./scripts/start-llama-servers.sh

# Run MCP server
python mcp-server/server.py --debug
```

---

## ğŸ”Œ Integration

### For Open Interpreter / Python Scripts

**HTTP API is already running on port 8090!**

```python
from hivemind_client import HiveMindClient

hive = HiveMindClient()

# Store context
hive.store_memory(
    context="Working on data analysis",
    files=["data.csv"],
    task="Generate insights"
)

# Recall context
context = hive.recall_memory()
print(context['context'])

# Get stats
stats = hive.get_stats()
print(f"Total sessions: {stats['total_sessions']}")
```

**Service Management**:
```bash
sudo systemctl status hive-mind-http   # Check status
sudo systemctl restart hive-mind-http  # Restart
sudo journalctl -u hive-mind-http -f   # View logs
```

**API Documentation**: http://localhost:8090/docs

### For Claude Code

Add to `~/.config/claude-code/mcp_config.json`:

```json
{
  "mcpServers": {
    "hive-mind": {
      "command": "/path/to/hive-mind/.venv/bin/python",
      "args": ["/path/to/hive-mind/mcp-server/server.py"],
      "env": {
        "CONFIG_PATH": "/path/to/hive-mind/config.yaml"
      }
    }
  }
}
```

Restart Claude Code and the memory system activates automatically! ğŸ‰

### Cross-Tool Context Sharing

Context stored via HTTP API is accessible via MCP protocol and vice versa:

```
Open Interpreter â†’ HTTP API â†’ Redis â† MCP Protocol â† Claude Code
```

**All tools share the same memory!**

---

## ğŸ“š Documentation

| Document | Description |
|----------|-------------|
| [README.md](README.md) | This file - overview and quick start |
| [QUICKSTART.md](QUICKSTART.md) | **Get started in 2 minutes!** |
| [DUAL_MODE_SETUP.md](DUAL_MODE_SETUP.md) | **Complete dual-mode guide (NEW!)** |
| [OPEN_INTERPRETER_INTEGRATION.md](docs/OPEN_INTERPRETER_INTEGRATION.md) | **Open Interpreter integration (NEW!)** |
| [SETUP_COMPLETE.md](SETUP_COMPLETE.md) | **Dual-mode setup summary (NEW!)** |
| [MCP_SERVER_READY.md](MCP_SERVER_READY.md) | Claude Code MCP integration |
| [CLUSTER_STATUS.md](CLUSTER_STATUS.md) | Redis cluster operations manual |
| [PERFORMANCE.md](PERFORMANCE.md) | Detailed benchmark results |
| [docs/ARCHITECTURE.md](docs/ARCHITECTURE.md) | Deep dive into system design |
| [docs/DEPLOYMENT.md](docs/DEPLOYMENT.md) | Production deployment guide |

---

## ğŸ—ºï¸ Roadmap

### âœ… Phase 1: Redis Cluster (COMPLETE)
- [x] 6-node cluster with auto-sharding
- [x] 3 Sentinels for high availability
- [x] AOF + RDB persistence
- [x] Password authentication
- [x] Performance: 12K+ ops/sec

### âœ… Phase 2: MCP Server (COMPLETE)
- [x] Cluster-aware client
- [x] Session management
- [x] Tool caching
- [x] Learning queue
- [x] Claude Code integration ready

### âœ… Phase 2.5: Local LLM Inference (COMPLETE)
- [x] Qwen2.5-Coder-7B (89 tok/s)
- [x] Qwen3-8B (74 tok/s)
- [x] ROCm GPU acceleration
- [x] Multi-model serving

### âœ… Phase 2.7: Dual-Mode Access (COMPLETE) ğŸ”¥
- [x] HTTP API server (FastAPI/Uvicorn)
- [x] RESTful endpoints for all operations
- [x] Systemd service (auto-start on boot)
- [x] Python client (`hivemind_client.py`)
- [x] Interactive API docs (Swagger UI)
- [x] Cross-tool context sharing
- [x] Open Interpreter integration ready

### âœ… Phase 4: Learning Pipeline (COMPLETE) ğŸ§ 
- [x] LoRA fine-tuning pipeline (PyTorch 2.9.1 + ROCm 7.12)
- [x] 10K+ foundation training dataset
- [x] Qwen2.5-0.5B validation (loss: 0.34, 9 min)
- [x] Qwen2.5-Coder-7B training (loss: 0.30, 1h 43min)
- [x] GGUF export with quantization (Q4_K_M, Q5_K_M, Q8_0)
- [x] Benchmark system with metrics tracking

### âœ… Phase 4.5: Smart Optimizer (COMPLETE) ğŸ§ 
- [x] Auto hardware detection (VRAM, GPU arch, BF16, Flash Attn)
- [x] Intelligent LoRA config (r, alpha based on model size)
- [x] Dynamic batch sizing based on available VRAM
- [x] Quality modes: fast / balanced / best
- [x] TorchAO integration (int4/int8 quantization)
- [x] Auto-select optimal export format

### ğŸš§ Phase 5: DELL Integration (PLANNED)
- [ ] Deploy llama-server on DELL (Alderlake + RDNA2 12GB)
- [ ] Add DELL as replica nodes to cluster
- [ ] Embedding service (sentence-transformers)
- [ ] Multi-machine context sharing

### ğŸ”® Phase 6: Continuous Learning (FUTURE)
- [ ] Collect interaction data from learning queue
- [ ] Scheduled re-training (weekly/monthly)
- [ ] A/B model evaluation
- [ ] Automated model deployment

---

## ğŸ’ª Hardware

### Current: BEAST
- **GPU**: AMD R9700 XT (32GB VRAM, RDNA4 gfx1201)
- **Storage**: 277 GB available
- **Network**: < 3ms latency to NAS
- **Role**: Primary workstation + cluster host

### NAS: Netgear ReadyNAS
- **CPU**: Intel Atom C3338 (2 cores @ 2.2GHz)
- **RAM**: 1.8GB
- **Storage**: 9.2 TB available
- **Network**: Dual gigabit NICs
- **Role**: Backup storage

### Future: DELL
- **CPU**: Alderlake Intel
- **RAM**: 32GB DDR
- **GPU**: RDNA2 12GB VRAM
- **Role**: Inference node + cluster replica

---

## ğŸ§ª Testing

### Run All Tests

```bash
cd /path/to/hive-mind
source .venv/bin/activate
python3 tests/test-hive-mind-stack.py
```

**Expected**: 8/8 tests pass âœ…

### Run Benchmarks

```bash
python3 tests/benchmark-hive-mind.py
```

---

## ğŸ¤ Contributing

Want to make Hive-Mind even better?

1. Fork the repo
2. Create a feature branch
3. Commit your changes
4. Push and open a Pull Request

---

## ğŸ“ License

MIT License

---

## ğŸ“Š Stats

**Lines of Code**: ~3,500
**Docker Containers**: 9 (6 Redis + 3 Sentinel)
**Services Running**: 12 (Redis + HTTP API + LLMs)
**Access Modes**: 2 (HTTP API + MCP Protocol)
**Development Time**: 1 incredible session + dual-mode enhancement
**Status**: ğŸ”¥ DUAL-MODE PRODUCTION READY ğŸ”¥

---

## ğŸ Why "Hive-Mind"?

Because like a bee hive:
- ğŸ **Distributed**: Multiple workers collaborating
- ğŸ¯ **Sweet**: Fast, efficient, production-ready
- ğŸ‘‘ **Organized**: Clear roles (masters, replicas, sentinels)
- ğŸ”„ **Resilient**: Auto-failover when workers fail
- ğŸ“š **Memory**: Collective knowledge that persists
- ğŸ”Œ **Universal**: Multiple access points (HTTP + MCP)
- ğŸ¤ **Collaborative**: Open Interpreter + Claude Code sharing context

Plus, it sounds cool. ğŸ˜

---

<div align="center">

**Built with â¤ï¸ and a lot of â˜•**

**Status**: âœ… PRODUCTION READY  
**Performance**: ğŸ”¥ EXCELLENT  
**Stability**: ğŸ›¡ï¸ ROCK SOLID

[â¬† Back to Top](#-hive-mind)

</div>
